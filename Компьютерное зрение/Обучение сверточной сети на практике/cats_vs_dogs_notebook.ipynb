{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1719ccb",
   "metadata": {},
   "source": [
    "# Dogs vs Cats â€” ready-to-run notebook\n",
    "\n",
    "This notebook is a ready-to-run solution template for the Kaggle **Dogs vs Cats Redux** contest.\n",
    "\n",
    "It uses transfer learning (VGG16) + data augmentation and includes training, prediction and submission cells.\n",
    "\n",
    "**Note:** to reach LogLoss < 0.3 you'll likely need to train longer and fine-tune on a full GPU instance (Kaggle/GPU or Colab Pro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check & imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "print('TensorFlow version:', tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c85ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and hyperparameters - edit these to match your environment\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20  # increase on Kaggle/GPU\n",
    "TRAIN_DIR = 'data/train_sample'   # change to '/kaggle/input/dogs-vs-cats-redux-kernels-edition/train' on Kaggle if needed\n",
    "TEST_DIR = 'data/test_sample'     # change accordingly\n",
    "OUTPUT_MODEL = 'cats_dogs_vgg16.h5'\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a32a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list a few files and visual check\n",
    "from glob import glob\n",
    "train_files = sorted(glob(os.path.join(TRAIN_DIR, '*.jpg')))\n",
    "test_files = sorted(glob(os.path.join(TEST_DIR, '*.jpg')))\n",
    "print('Train sample files:', len(train_files))\n",
    "print('Test sample files:', len(test_files))\n",
    "# show 6 examples\n",
    "import cv2\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "for i, p in enumerate(train_files[:6]):\n",
    "    ax = fig.add_subplot(1,6,i+1)\n",
    "    img = cv2.imread(p)[...,::-1]\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    ax.imshow(img.astype('uint8'))\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b738661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators (balanced class mode using filenames naming convention 'dog.123.jpg' and 'cat.123.jpg')\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.vgg16.preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.05  # small val split; for final eval use separate holdout\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\n",
    "\n",
    "# flow_from_directory expects subfolders like train/cat and train/dog. If your TRAIN_DIR already is the folder containing cat/ and dog/, pass its parent.\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    os.path.dirname(TRAIN_DIR) if os.path.isdir(os.path.dirname(TRAIN_DIR)) else TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    os.path.dirname(TRAIN_DIR) if os.path.isdir(os.path.dirname(TRAIN_DIR)) else TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=SEED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b6377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: if your TRAIN_DIR contains files like 'dog.123.jpg' and 'cat.456.jpg' (no subfolders),\n",
    "# use a custom generator that reads filenames and yields batches (uncomment and adapt if needed).\n",
    "'''\n",
    "from tensorflow.keras.utils import Sequence\n",
    "class FileSequence(Sequence):\n",
    "    def __init__(self, files, batch_size, img_size, shuffle=True):\n",
    "        self.files = files\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.files) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_files = self.files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        X = np.zeros((len(batch_files), self.img_size[0], self.img_size[1], 3), dtype=np.float32)\n",
    "        y = np.zeros((len(batch_files),), dtype=np.float32)\n",
    "        for i, p in enumerate(batch_files):\n",
    "            img = cv2.imread(p)[...,::-1]\n",
    "            img = cv2.resize(img, self.img_size)\n",
    "            img = tf.keras.applications.vgg16.preprocess_input(img)\n",
    "            X[i] = img\n",
    "            y[i] = 1.0 if os.path.basename(p).startswith('dog') else 0.0\n",
    "        return X, y\n",
    "\n",
    "# seq = FileSequence(train_files[val_count:], BATCH_SIZE, IMG_SIZE)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff05913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model: VGG16 base + small classifier head\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "out = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = models.Model(inputs=base_model.input, outputs=out, name='vgg16_transfer')\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01169a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "cb = [\n",
    "    callbacks.ModelCheckpoint(OUTPUT_MODEL, save_best_only=True, monitor='val_loss'),\n",
    "    callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "# Train - if using flow_from_directory above (ensure directories structured as cat/ and dog/)\n",
    "steps_per_epoch = max(1, train_generator.samples // BATCH_SIZE)\n",
    "validation_steps = max(1, val_generator.samples // BATCH_SIZE)\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=cb\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning: unfreeze top VGG blocks and train with lower LR\n",
    "# WARNING: only do this on a GPU and after initial training converged\n",
    "for layer in base_model.layers[-6:]:\n",
    "    layer.trainable = True\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "ft_history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=10,\n",
    "    callbacks=cb\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfd2fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test files and create submission (adapt depending on Kaggle test set naming)\n",
    "import pandas as pd\n",
    "# If test images are in a single folder, create a generator\n",
    "from glob import glob\n",
    "test_files = sorted(glob(os.path.join(TEST_DIR, '*.jpg')))\n",
    "def predict_single(model, files):\n",
    "    preds = []\n",
    "    ids = []\n",
    "    for p in files:\n",
    "        img = cv2.imread(p)[...,::-1]\n",
    "        img = cv2.resize(img, IMG_SIZE)\n",
    "        x = tf.keras.applications.vgg16.preprocess_input(img)\n",
    "        x = np.expand_dims(x, 0)\n",
    "        pred = float(model.predict(x)[0][0])\n",
    "        preds.append(pred)\n",
    "        m = re.search(r'(\\d+)\\.jpg$', p)\n",
    "        ids.append(m.group(1) if m else os.path.basename(p))\n",
    "    return ids, preds\n",
    "\n",
    "ids, preds = predict_single(model, test_files)\n",
    "\n",
    "sub = pd.DataFrame({'id': ids, 'label': preds})\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "print('Saved submission.csv with', len(sub), 'rows.')\n",
    "sub.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc022167",
   "metadata": {},
   "source": [
    "## How to get a low LogLoss (practical tips)\n",
    "\n",
    "- Train longer with more data augmentation and strong regularization.\n",
    "- Use class weight balancing if your dataset is imbalanced.\n",
    "- Fine-tune the top VGG blocks on a GPU.\n",
    "- Use Test Time Augmentation (TTA) and ensembling of models.\n",
    "\n",
    "---\n",
    "\n",
    "**After you submit to Kaggle**, paste your Leaderboard LogLoss value below in the next cell. The task is considered passed when LogLoss < 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065618b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Paste your Kaggle Leaderboard LogLoss here after successful submit\n",
    "kaggle_logloss = None  # e.g. 0.28902\n",
    "print('Kaggle LogLoss:', kaggle_logloss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f662c",
   "metadata": {},
   "source": [
    "### Final notes\n",
    "\n",
    "This notebook is a starting point. To actually reach the required LogLoss < 0.3 you will likely need:\n",
    "\n",
    "- Full dataset (not tiny sample).\n",
    "- More epochs (50+), progressive resizing, lr schedules.\n",
    "- Ensembling (e.g., VGG16 + EfficientNet variants) and TTA.\n",
    "\n",
    "Good luck â€” run this on Kaggle with GPU and paste the final LogLoss in the cell above."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
